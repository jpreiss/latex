@book{sutton-barto,
  author    = {Richard S. Sutton and
               Andrew G. Barto},
  title     = {Reinforcement learning - an introduction},
  series    = {Adaptive computation and machine learning},
  publisher = {{MIT} Press},
  edition   = 2,
  year      = {2018}
}

@inproceedings{schulman-trpo,
  author    = {John Schulman and
               Sergey Levine and
               Pieter Abbeel and
               Michael I. Jordan and
               Philipp Moritz},
  title     = {Trust Region Policy Optimization},
  booktitle = {{ICML}},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {37},
  pages     = {1889--1897},
  publisher = {JMLR.org},
  year      = {2015}
}

@article{openai-gym,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {Open{AI} {G}ym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016}
}

@misc{openai-baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@article{duan-benchmarking-DRL,
  author    = {Yan Duan and
               Xi Chen and
               Rein Houthooft and
               John Schulman and
               Pieter Abbeel},
  title     = {Benchmarking Deep Reinforcement Learning for Continuous Control},
  journal   = {CoRR},
  volume    = {abs/1604.06778},
  year      = {2016}
}

@inproceedings{todorov-mujoco,
  author    = {Emanuel Todorov and
               Tom Erez and
               Yuval Tassa},
  title     = {MuJoCo: {A} physics engine for model-based control},
  booktitle = {{IROS}},
  pages     = {5026--5033},
  publisher = {{IEEE}},
  year      = {2012}
}

@article{zhang-study-on-overfitting,
  author    = {Chiyuan Zhang and
               Oriol Vinyals and
               R{\'{e}}mi Munos and
               Samy Bengio},
  title     = {A Study on Overfitting in Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1804.06893},
  year      = {2018}
}

@inproceedings{teh-distral,
  author    = {Yee Whye Teh and
               Victor Bapst and
               Wojciech M. Czarnecki and
               John Quan and
               James Kirkpatrick and
               Raia Hadsell and
               Nicolas Heess and
               Razvan Pascanu},
  title     = {Distral: Robust multitask reinforcement learning},
  booktitle = {{NIPS}},
  pages     = {4499--4509},
  year      = {2017}
}

@article{duan-rl2,
  author    = {Yan Duan and
               John Schulman and
               Xi Chen and
               Peter L. Bartlett and
               Ilya Sutskever and
               Pieter Abbeel},
  title     = {RL{\textdollar}{\^{}}2{\textdollar}: Fast Reinforcement Learning via
               Slow Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1611.02779},
  year      = {2016}
}

@article{OpenAI-dexterous,
  author    = {OpenAI},
  title     = {Learning Dexterous In-Hand Manipulation},
  journal   = {CoRR},
  volume    = {abs/1808.00177},
  year      = {2018}
}

@inproceedings{calandra-inverse-dynamics-contacts-icra15,
  author    = {Roberto Calandra and
               Serena Ivaldi and
               Marc Peter Deisenroth and
               Elmar A. R{\"{u}}ckert and
               Jan Peters},
  title     = {Learning inverse dynamics models with contacts},
  booktitle = {{ICRA}},
  pages     = {3186--3191},
  publisher = {{IEEE}},
  year      = {2015}
}

@article{shelhamer-loss-own-reward-arxiv16,
  author    = {Evan Shelhamer and
               Parsa Mahmoudieh and
               Max Argus and
               Trevor Darrell},
  title     = {Loss is its own Reward: Self-Supervision for Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1612.07307},
  year      = {2016}
}

@inproceedings{haarnoja-soft-actor-critic,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  booktitle = {{ICML}},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {80},
  pages     = {1856--1865},
  publisher = {JMLR.org},
  year      = {2018}
}

@article{lillicrap-DDPG,
  author    = {Timothy P. Lillicrap and
               Jonathan J. Hunt and
               Alexander Pritzel and
               Nicolas Heess and
               Tom Erez and
               Yuval Tassa and
               David Silver and
               Daan Wierstra},
  title     = {Continuous control with deep reinforcement learning},
  journal   = {CoRR},
  volume    = {abs/1509.02971},
  year      = {2015}
}

@article{held-goal-generation,
  author    = {David Held and
               Xinyang Geng and
               Carlos Florensa and
               Pieter Abbeel},
  title     = {Automatic Goal Generation for Reinforcement Learning Agents},
  journal   = {CoRR},
  volume    = {abs/1705.06366},
  year      = {2017}
}

@article{actor-mimic,
  author    = {Emilio Parisotto and
               Lei Jimmy Ba and
               Ruslan Salakhutdinov},
  title     = {Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1511.06342},
  year      = {2015}
}

@article{fujimoto-actor-critic-approx-error,
  author    = {Scott Fujimoto and
               Herke van Hoof and
               Dave Meger},
  title     = {Addressing Function Approximation Error in Actor-Critic Methods},
  journal   = {CoRR},
  volume    = {abs/1802.09477},
  year      = {2018}
}

@article{mania-random-search-RL,
  author    = {Horia Mania and
               Aurelia Guy and
               Benjamin Recht},
  title     = {Simple random search provides a competitive approach to reinforcement
               learning},
  journal   = {CoRR},
  volume    = {abs/1803.07055},
  year      = {2018}
}

@inproceedings{hausman-embedding-2018,
  author = {Karol Hausman and
            Jost Tobias Springenberg and
            Ziyu Wang and
            Nicolas Heess and
            Martin Riedmiller},
  title = {Learning an Embedding Space for Transferable Robot Skills},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2018},
}

@inproceedings{Q-learning-nearest-neighbors,
  author    = {Devavrat Shah and
               Qiaomin Xie},
  title     = {Q-learning with Nearest Neighbors},
  booktitle = {NeurIPS},
  pages     = {3115--3125},
  year      = {2018}
}

@article{finn-gan-IRL-connection,
  author    = {Chelsea Finn and
               Paul F. Christiano and
               Pieter Abbeel and
               Sergey Levine},
  title     = {A Connection between Generative Adversarial Networks, Inverse Reinforcement
               Learning, and Energy-Based Models},
  journal   = {CoRR},
  volume    = {abs/1611.03852},
  year      = {2016}
}

@article{bartlett2002estimation,
  title={Estimation and approximation bounds for gradient-based reinforcement learning},
  author={Bartlett, Peter L and Baxter, Jonathan},
  journal={Journal of Computer and System Sciences},
  volume={64},
  number={1},
  pages={133--150},
  year={2002},
  publisher={Academic Press}
}

@article{greensmith2004variance,
  title={Variance reduction techniques for gradient estimates in reinforcement learning},
  author={Greensmith, Evan and Bartlett, Peter L and Baxter, Jonathan},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Nov},
  pages={1471--1530},
  year={2004}
}

@inproceedings{lau2012importance,
  title={The importance of variance reduction in policy gradient method},
  author={Lau, Tak Kit and Liu, Yun-hui},
  booktitle={American Control Conference (ACC), 2012},
  pages={1376--1381},
  year={2012},
  organization={IEEE}
}


@misc{mao2018variance,
Author = {Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},
Title = {Variance Reduction for Reinforcement Learning in Input-Driven Environments},
Year = {2018},
Eprint = {arXiv:1807.02264},
}

@inproceedings{tokui2017evaluating,
  title={Evaluating the Variance of Likelihood-Ratio Gradient Estimators},
  author={Tokui, Seiya and Sato, Issei},
  booktitle={International Conference on Machine Learning},
  pages={3414--3423},
  year={2017}
}

@article{williams-REINFORCE,
  author    = {Ronald J. Williams},
  title     = {Simple Statistical Gradient-Following Algorithms for Connectionist
               Reinforcement Learning},
  journal   = {Machine Learning},
  volume    = {8},
  pages     = {229--256},
  year      = {1992}
}

@inproceedings{henderson-deep-rl-that-matters,
  author    = {Peter Henderson and
               Riashat Islam and
               Philip Bachman and
               Joelle Pineau and
               Doina Precup and
               David Meger},
  title     = {Deep Reinforcement Learning That Matters},
  booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  pages     = {3207--3214},
  publisher = {{AAAI} Press},
  year      = {2018},
}

@article{schulman-ppo,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017}
}

@article{duan-benchmarking,
  author    = {Yan Duan and
               Xi Chen and
               Rein Houthooft and
               John Schulman and
               Pieter Abbeel},
  title     = {Benchmarking Deep Reinforcement Learning for Continuous Control},
  journal   = {CoRR},
  volume    = {abs/1604.06778},
  year      = {2016}
}

@article{levine-rl-inference,
  author    = {Sergey Levine},
  title     = {Reinforcement Learning and Control as Probabilistic Inference: Tutorial
               and Review},
  journal   = {CoRR},
  volume    = {abs/1805.00909},
  year      = {2018}
}

